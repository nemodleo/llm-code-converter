import argparse
import re
from difflib import unified_diff
from difflib import SequenceMatcher
from typing import Optional, Union, List, Dict, Any, Tuple

from pydantic import BaseModel, Field, model_validator

from aiconvertor.agent import Agent
from aiconvertor.java_utils_tree_sitter import split_java_code
from aiconvertor.java_utils_tree_sitter import matches_regardless_of_spacing
from aiconvertor.java_utils_tree_sitter import get_ast
from aiconvertor.prompt_handler import PromptHandler
from aiconvertor.prompt_handler import load_contexts
from aiconvertor.rag.retriever import ApiRetriever
from aiconvertor.incontext.retriever import CaseRetriever
from snucse_2501_aiconvertor.aiconvertor.dependency.vo_generator import VOGenerator


class ConversionConfig(BaseModel):
    """ë³€í™˜ ì„¤ì •ì„ ìœ„í•œ Pydantic ëª¨ë¸"""
    model: str = Field(default='qwen2.5-coder:7b', description='ì‚¬ìš©í•  ëª¨ë¸')
    mode: str = Field(default='module', description='ë³€í™˜ ëª¨ë“œ: module, page, line')
    use_diff: bool = Field(default=False, description='diff ì •ë³´ í¬í•¨ ì—¬ë¶€')
    use_reflextion: bool = Field(default=False, description='í”¼ë“œë°± ê¸°ë°˜ ë°˜ë³µ ê°œì„  ì‚¬ìš© ì—¬ë¶€')
    use_prompt_normalization: bool = Field(default=False, description='í”„ë¡¬í”„íŠ¸ ì •ê·œí™” ì‚¬ìš© ì—¬ë¶€')
    use_prefix_output: bool = Field(default=True, description='```java ì½”ë“œ ë¸”ë¡ ì‚¬ìš© ì—¬ë¶€')
    use_vo_generator: bool = Field(default=False, description='VO ìƒì„±ê¸° ì‚¬ìš© ì—¬ë¶€')
    vo_package: Optional[str] = Field(default=None, description='VO íŒ¨í‚¤ì§€')
    vo_class_name: Optional[str] = Field(default=None, description='VO í´ë˜ìŠ¤ ì´ë¦„')
    project_root: Optional[str] = Field(default=None, description='í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ')
    vo_file: Optional[str] = Field(default=None, description='VO íŒŒì¼ ê²½ë¡œ')
    use_api_rag: bool = Field(default=False, description='Proworks5 api RAG ì‚¬ìš© ì—¬ë¶€')
    use_case_rag: bool = Field(default=False, description='Case RAG ì‚¬ìš© ì—¬ë¶€')
    max_line_limit_offset: Optional[int] = Field(default=None, description='Agent ì‘ë‹µ ë¼ì¸ ìˆ˜ ì œí•œ ì˜¤í”„ì…‹')
    skip_non_map: bool = Field(default=False, description='Map ê´€ë ¨ ì½”ë“œê°€ ì—†ëŠ” ê²½ìš° ë³€í™˜ ê±´ë„ˆë›°ê¸°')
    context: Optional[str] = Field(default=None, description='ì»¨í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ')
    java: str = Field(default="data/samples/file_sample_original/SampleTaskServiceImpl_in.java", description='ì…ë ¥ Java íŒŒì¼ ê²½ë¡œ')
    gt: str = Field(default="data/samples/file_sample_original/SampleTaskServiceImpl_out.java", description='Ground truth Java íŒŒì¼ ê²½ë¡œ')
    iterations: int = Field(default=1, description='ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜')
    verbose: bool = Field(default=False, description='ìƒì„¸ ì¶œë ¥ ëª¨ë“œ')

    @model_validator(mode='after')
    def validate_vo_options(self):
        """VO ê´€ë ¨ ì˜µì…˜ ìœ íš¨ì„± ê²€ì‚¬"""
        if self.use_vo_generator and self.vo_file:
            raise ValueError("use_vo_generator ì™€ vo_file ì€ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return self

    class Config:
        """Pydantic ì„¤ì •"""
        validate_assignment = True
        extra = 'forbid'  # ì •ì˜ë˜ì§€ ì•Šì€ í•„ë“œ ê¸ˆì§€


COLOR_MAP = {
    "reset": "\033[0m",
    "cyan": "\033[96m",
    "green": "\033[92m",
    "magenta": "\033[95m",
    "yellow": "\033[93m",
}

def print_color(body, color):
    color_code = COLOR_MAP.get(color, COLOR_MAP["reset"])
    print(color_code + body + COLOR_MAP["reset"])


class AIConverter:
    """AIë¡œ Codeë¥¼ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self,
                 agent: Agent,
                 use_diff: bool = False,
                 use_reflextion: bool = False,
                 use_prompt_normalization: bool = False,
                 use_prefix_output: bool = False,
                 use_vo_generator: bool = False,
                 vo_file: str = None,
                 use_api_rag: bool = False,
                 use_case_rag: bool = False,
                 iterations: int = 1,
                 skip_non_map: bool = False,
                 max_line_limit_offset: int = None,
                 project_root: str = None,
                 vo_package: str = None,
                 vo_class_name: str = None):
        self.agent = agent
        self.prompt_handler = PromptHandler()
        self.use_diff = use_diff
        self.use_reflextion = use_reflextion
        self.use_vo_generator = use_vo_generator
        self.vo_file = vo_file
        self.use_api_rag = use_api_rag
        self.use_case_rag = use_case_rag
        self.use_prompt_normalization = use_prompt_normalization
        self.use_prefix_output = use_prefix_output
        self.iterations = iterations
        self.skip_non_map = skip_non_map
        self.max_line_limit_offset = max_line_limit_offset

        self.vo_code: str | None = None
        if use_vo_generator:
            vo_generator = VOGenerator(
                project_root=project_root,
                vo_package=vo_package,
                vo_class_name=vo_class_name
            )
            self.vo_code = vo_generator.generate_vo_class()
        elif vo_file:
            self.vo_code = load_contexts(vo_file)
        else:
            pass


        self.api_retriever = None
        if use_api_rag:
            _embedding_model_name = "microsoft/codebert-base"
            self.api_retriever = ApiRetriever(
                vectorstore_path=f"data/db/proworks5_vectorstore_{_embedding_model_name.split('/')[-1]}",
                embedding_model_name=_embedding_model_name
            )

        self.case_retriever = None
        if use_case_rag:
            self.case_retriever = CaseRetriever()
            self.case_retriever.load_embedding_data()

    
    def load_java_files(self, 
                       input_java_path: str,
                       context_path: str = None,
                       gt_java_path: str = None) -> dict[str, any]:
        """Java íŒŒì¼ë“¤ê³¼ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ"""
        
        # ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
        if context_path:
            contexts = load_contexts(context_path)
        else:
            contexts = ""
        # TODO build contexts
        
        # Java ì½”ë“œ ë¡œë“œ
        java_code = load_contexts(input_java_path)
        
        # Ground truth ì½”ë“œ ë¡œë“œ (ì„ íƒì )
        gt_java_code = None
        if gt_java_path:
            gt_java_code = load_contexts(gt_java_path)
        else:
            gt_java_code = java_code  # fallback
        
        return {
            'contexts': contexts,
            'java_code': java_code,
            'gt_java_code': gt_java_code,
        }
    
    def print_extracted_methods(self, data: dict[str, any]):
        """ì¶”ì¶œëœ ë©”ì„œë“œë“¤ ì¶œë ¥"""
        
        fn_names = data['function_names']
        gt_fn_bodies = data['gt_function_bodies']
        
        for fn in fn_names:
            print(f"\nFunction: {fn}")
            print(gt_fn_bodies.get(fn, "[Not Found in GT]"))

    
    # ì½”ë“œ íŒŒì‹±: ì•ìª½ ì£¼ì„, ë“¤ì—¬ì“°ê¸°, ì‹¤ì œ ì½”ë“œ, ë’¤ìª½ ì£¼ì„ ë¶„ë¦¬
    def parse_code_structure(self, code_text):
        lines = code_text.split('\n')
        
        # ì•ìª½ ì£¼ì„ê³¼ ê³µë°± ë¼ì¸ ì°¾ê¸°
        prefix_lines = []
        start_idx = 0
        for i, line in enumerate(lines):
            stripped = line.strip()
            if stripped == '' or stripped.startswith('//') or stripped.startswith('/*') or stripped.endswith('*/'):
                prefix_lines.append(line)
                start_idx = i + 1
            else:
                break
        
        # ë’¤ìª½ ì£¼ì„ê³¼ ê³µë°± ë¼ì¸ ì°¾ê¸°
        postfix_lines = ['```java']
        end_idx = len(lines)
        for i in range(len(lines) - 1, -1, -1):
            stripped = lines[i].strip()
            if stripped == '' or stripped.startswith('//') or stripped.startswith('/*') or stripped.endswith('*/'):
                postfix_lines.insert(0, lines[i])
                end_idx = i
            else:
                break
        
        # ì‹¤ì œ ì½”ë“œ ë¶€ë¶„
        code_lines = lines[start_idx:end_idx]
        
        # ë“¤ì—¬ì“°ê¸° ì¶”ì¶œ (ì²« ë²ˆì§¸ ë¹„ì–´ìˆì§€ ì•Šì€ ì½”ë“œ ë¼ì¸ì—ì„œ)
        indent_prefix = ""
        for line in code_lines:
            if line.strip():
                for char in line:
                    if char in [' ', '\t']:
                        indent_prefix += char
                    else:
                        break
                break
        
        # ì •ê·œí™”ëœ ì½”ë“œ (ë“¤ì—¬ì“°ê¸° ì œê±°)
        normalized_lines = []
        for line in code_lines:
            if line.strip():
                # ê³µí†µ ë“¤ì—¬ì“°ê¸° ì œê±°
                if line.startswith(indent_prefix):
                    normalized_lines.append(line[len(indent_prefix):])
                else:
                    normalized_lines.append(line.lstrip())
            else:
                normalized_lines.append('')
        
        normalize_code = '\n'.join(normalized_lines).strip()
        prefix_output = '\n'.join(prefix_lines)
        postfix_output = '\n'.join(postfix_lines)
        
        return normalize_code, indent_prefix, prefix_output, postfix_output

    def recover_original_code(self, normalize_code, indent_prefix, prefix_output, postfix_output):
        # ì •ê·œí™”ëœ ì½”ë“œì— ë“¤ì—¬ì“°ê¸° ë‹¤ì‹œ ì ìš©
        indented_lines = []
        for line in normalize_code.split('\n'):
            if line.strip():
                indented_lines.append(indent_prefix + line)
            else:
                indented_lines.append('')
        
        # ì „ì²´ ì¡°í•©
        parts = []
        if prefix_output:
            parts.append(prefix_output)
        if indented_lines:
            parts.append('\n'.join(indented_lines))
        if postfix_output:
            parts.append(postfix_output)
        
        return '\n'.join(parts)

    def _build_contexts(self, contexts: str, code: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ"""
        if self.vo_code:
            vo_code_prompt = f"<vo_class>\n{self.vo_code}\n</vo_class>\n\n"
            contexts += vo_code_prompt

        if self.use_api_rag:
            api_prompt = self.api_retriever.get_prompt(code)
            if api_prompt is not None:
                contexts += api_prompt

        if self.use_case_rag:
            case_prompt = self.case_retriever.get_prompt(code)
            if case_prompt is not None:
                contexts += case_prompt

        return contexts

    def _generate_diff(self, original_code: str, candidate_code: str) -> str:
        """diff ìƒì„± (ê³µí†µ í•¨ìˆ˜)"""
        return '\n'.join(unified_diff(
            original_code.splitlines(),
            candidate_code.splitlines(),
            fromfile='input',
            tofile='candidate',
            lineterm=''
        ))

    def _call_agent_and_extract_code(self, prompt: str, stage: str, max_lines: int) -> str:
        """agent í˜¸ì¶œ ë° ì½”ë“œ ì¶”ì¶œ (ê³µí†µ í•¨ìˆ˜)"""
        response = self.agent(prompt, clear_messages=True, max_lines=max_lines)
        extracted_code = self.prompt_handler.extract_code_from_response(response, self.use_prefix_output)
        self._print_prompt_response(prompt, response, stage, extracted_code)
        return extracted_code

    def _generate_feedback(self, contexts: str, code: str, prev_output: str, max_lines: int) -> str:
        """í”¼ë“œë°± ìƒì„± ì „ìš© í•¨ìˆ˜"""
        diff_text = None
        
        # diff ìƒì„± (í•„ìš”í•œ ê²½ìš°)
        if self.use_diff:
            diff_text = self._generate_diff(code, prev_output)
        
        # í”¼ë“œë°± ìƒì„± í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
        prompt = self.prompt_handler.build_feedback_prompt(
            contexts, prev_output, diff_text, self.use_diff
        )
        
        # agent í˜¸ì¶œ ë° í”¼ë“œë°± ì¶”ì¶œ
        response = self.agent(prompt, clear_messages=True, max_lines=max_lines)
        feedback = self.prompt_handler.extract_feedback_from_response(response)
        self._print_prompt_response(prompt, response, "Feedback Generation", None)
        
        return feedback

    def _generate_diff_if_needed(self, code: str, prev_output: str) -> str | None:
        """diff ìƒì„± (í•„ìš”í•œ ê²½ìš°ì—ë§Œ)"""
        return self._generate_diff(code, prev_output) if self.use_diff else None

    def _perform_code_correction(self, contexts: str, code: str, prev_output: str, 
                                feedback: str | None, stage: str, max_lines: int) -> str:
        """ì½”ë“œ ìˆ˜ì • ê³µí†µ í•¨ìˆ˜ (í”¼ë“œë°± ìˆ/ì—† ëª¨ë‘ ì²˜ë¦¬)"""
        
        # diff ìƒì„± (í•„ìš”í•œ ê²½ìš°)
        diff_text = self._generate_diff_if_needed(code, prev_output)
        
        # ìˆ˜ì • í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
        prompt = self.prompt_handler.build_correction_prompt(
            contexts, prev_output, code,
            feedback, diff_text, self.use_diff
        )
        
        return self._call_agent_and_extract_code(prompt, stage, max_lines)

    def _perform_initial_conversion(self, contexts: str, normalize_code: str, prefix_output: str, max_lines: int) -> str:
        """ì²« ë²ˆì§¸ ì‹œë„: ì´ˆê¸° ë³€í™˜"""
        prompt = self.prompt_handler.build_initial_prompt(
            contexts, normalize_code, prefix_output
        )
        
        return self._call_agent_and_extract_code(prompt, "Initial Conversion", max_lines)

    def _perform_reflexion_conversion(self, contexts: str, code: str, prev_output: str, max_lines: int) -> str:
        """ë‘ ë²ˆì§¸ ì‹œë„: í”¼ë“œë°± ìƒì„± ë° ê¸°ë°˜ ìˆ˜ì • (reflexion ëª¨ë“œ)"""
        
        # í”¼ë“œë°± ìƒì„±
        feedback = self._generate_feedback(contexts, code, prev_output, max_lines)

        # í”¼ë“œë°± ê¸°ë°˜ ìˆ˜ì •
        return self._perform_code_correction(
            contexts, code, prev_output, feedback, 
            "Feedback-based Correction", max_lines
        )

    def _perform_iterative_improvement(self, contexts: str, code: str, prev_output: str, max_lines: int) -> str:
        """ë°˜ë³µ ê°œì„ : diff ì •ë³´ ê¸°ë°˜ ìˆ˜ì •"""
        
        # í”¼ë“œë°± ì—†ì´ ìˆ˜ì •
        return self._perform_code_correction(
            contexts, code, prev_output, None, 
            "Iterative Improvement", max_lines
        )

    def convert_code(self,
                     contexts: str,
                     code: str,
                     gt_code: str) -> dict[str, any]:
        """ë‹¨ì¼ ë³€í™˜"""    
        
        # Map ê´€ë ¨ ì½”ë“œê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì½”ë“œ ê·¸ëŒ€ë¡œ ë¦¬í„´ (ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°)
        if self.skip_non_map:
            if not re.search(r"\bmap\.(get|put|remove)\b|Map", code):
                print("â„¹ï¸  No map operations found. Returning original code.")
                return {
                    'original_code': code,
                    'converted_code': code,
                    'is_correct': True,  # ë³€í™˜ì´ í•„ìš”ì—†ìœ¼ë¯€ë¡œ ì •ë‹µìœ¼ë¡œ ì²˜ë¦¬
                    'iterations': 0,
                    'ground_truth': gt_code,
                    'skipped': True
                }
        
        if self.use_prompt_normalization:
            normalize_code, indent_prefix, prefix_output, postfix_output = self.parse_code_structure(code)
            prefix_output = "\n" +  postfix_output if self.use_prefix_output else "\n"
        else:
            normalize_code = code
            indent_prefix = ""
            prefix_output = "\n" + "```java\n" if self.use_prefix_output else "\n"
            postfix_output = ""

        diff_text = None
        prev_output = None
        
        # ë¼ì¸ ìˆ˜ ì œí•œ ê³„ì‚°
        max_lines = None
        if self.max_line_limit_offset is not None:
            code_lines = len(normalize_code.splitlines())
            max_lines = code_lines + self.max_line_limit_offset
        
        for i in range(self.iterations):
            print(f"\n--- Iteration {i + 1} ---")
            
            if prev_output is None:
                prev_output = self._perform_initial_conversion(contexts, normalize_code, prefix_output, max_lines)
            elif self.use_reflextion:
                prev_output = self._perform_reflexion_conversion(contexts, code, prev_output, max_lines)
            else:
                prev_output = self._perform_iterative_improvement(contexts, code, prev_output, max_lines)

            # í”„ë¡¬í”„íŠ¸ ì •ê·œí™” ì‚¬ìš© ì‹œ ì›ë³¸ ì½”ë“œë¡œ ë³µêµ¬
            if self.use_prompt_normalization:
                prev_output = self.recover_original_code(prev_output, indent_prefix, prefix_output, postfix_output)

            # ê²°ê³¼ í‰ê°€
            is_correct = self._evaluate_output(prev_output, gt_code)['exact_match']
            if is_correct:
                break
        
        # ìµœì¢… ê²°ê³¼
        print("\n" + "="*50)
        print("Final corrected method:")
        print(prev_output)
        
        # ìµœì¢… í‰ê°€
        is_correct = self._evaluate_output(prev_output, gt_code)['exact_match']
        
        return {
            'original_code': code,
            'converted_code': prev_output,
            'is_correct': is_correct,
            'iterations': i + 1,
            'ground_truth': gt_code
        }
    
    def convert_all_modules(self, data: dict[str, any], skip_non_map_code: bool = False) -> list[dict[str, any]]:
        """ëª¨ë“  ëª¨ë“ˆ ë³€í™˜ (Module ë‹¨ìœ„)"""
        
        results = []
        contexts = data['contexts']
        java_code = data['java_code']
        gt_java_code = data['gt_java_code']

        # split java_code by module 
        java_codes = split_java_code(java_code)
        java_codes = [unit['content'] for unit in java_codes if 'content' in unit]
        # split gt_java_code by module
        gt_java_codes = split_java_code(gt_java_code)
        gt_java_codes = [unit['content'] for unit in gt_java_codes if 'content' in unit]
        if len(java_codes) != len(gt_java_codes):
            print(f"âš ï¸  Module count mismatch: {len(java_codes)} vs {len(gt_java_codes)}")
            if len(gt_java_codes) < len(java_codes):
                # gt_java_codesê°€ ë¶€ì¡±í•œ ê²½ìš°, gt_java_codesë¥¼ java_codesì˜ ê¸¸ì´ì— ë§ì¶°ì„œ ì±„ì›€
                gt_java_codes += [''] * (len(java_codes) - len(gt_java_codes))
            elif len(java_codes) < len(gt_java_codes):
                # java_codesê°€ ë¶€ì¡±í•œ ê²½ìš°, java_codesë¥¼ gt_java_codesì˜ ê¸¸ì´ì— ë§ì¶°ì„œ ì±„ì›€
                java_codes += [''] * (len(gt_java_codes) - len(java_codes))

        print(f"ğŸš€ Converting {len(java_codes)} Modules... (Module Mode)")

        for i, (java_module_code, gt_java_module_code) in enumerate(zip(java_codes, gt_java_codes)):
            print(f"\n{'='*60}")
            print(f"Converting module {i+1}/{len(java_codes)}")
            print(f"{'='*60}")
            
            try:
                contexts = self._build_contexts(contexts, java_module_code)

                result = self.convert_code(
                    contexts, java_module_code, gt_java_module_code
                )
                results.append(result)

            except Exception as e:
                print(f"âŒ Error converting module {i+1}/{len(java_codes)}: {e}")
                results.append({
                    'module_code': java_module_code,
                    'error': str(e),
                    'is_correct': False
                })
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        self._print_summary(results)
        
        return results

    def convert_whole_page(self, data: dict[str, any]) -> dict[str, any]:
        """ì „ì²´ í˜ì´ì§€ ë³€í™˜ (Page ë‹¨ìœ„)"""
        contexts = data['contexts']
        java_code = data['java_code']
        gt_java_code = data['gt_java_code']
        
        print(f"ğŸ“„ Converting entire page... (Page Mode)")
        print(f"   Original code length: {len(java_code)} characters")

        contexts = self._build_contexts(contexts, java_code)

        result = self.convert_code(contexts, java_code, gt_java_code)

        self._print_page_summary(result)
        
        return result
    
    def _print_prompt_response(self, prompt: str, response: str, stage: str, parsed_code: str = None):
        """í”„ë¡¬í”„íŠ¸, ì‘ë‹µ, íŒŒì‹±ëœ ì½”ë“œ ì¶œë ¥"""
        print(f"\n{stage}:")
        print("Prompt:")
        print_color(prompt, "green")
        print()
        print("Response:")
        print_color(response, "cyan")
        print()
        
        if parsed_code is not None:
            # ìƒˆë¡œìš´ ìƒ‰ìƒ ì¶”ê°€
            print("Parsed Code:")
            print_color(parsed_code, "magenta")
            print()

    def _evaluate_output(self, output: str, gt_code: str) -> dict:
        """ì¶œë ¥ ê²°ê³¼ í‰ê°€ (í˜ì´ì§€ ë‹¨ìœ„, ë‹¤ì¤‘ ë©”íŠ¸ë¦­ í¬í•¨)"""

        def remove_comments(code):
            # ë¼ì¸ ì£¼ì„ + ë¸”ë¡ ì£¼ì„ ì œê±°
            code = re.sub(r'//.*?$', '', code, flags=re.MULTILINE)
            code = re.sub(r'/\*.*?\*/', '', code, flags=re.DOTALL)
            return code

        def contains_vo_method(code):
            return bool(re.search(r'\b(set|get)[A-Z]\w*\s*\(', code))

        def contains_map_ops(code):
            return bool(re.search(r'\bmap\.(get|put|remove)\s*\(', code, flags=re.IGNORECASE))

        result = {}

        # 1. Exact match (ê³µë°± ë¬´ì‹œ)
        exact_match = matches_regardless_of_spacing(output, gt_code)
        result['exact_match'] = exact_match

        # 2. Comment-ignored match
        output_no_comments = remove_comments(output)
        gt_no_comments = remove_comments(gt_code)
        comment_ignored_match = matches_regardless_of_spacing(output_no_comments, gt_no_comments)
        result['comment_ignored_match'] = comment_ignored_match

        # 3. VO getter/setter íŒ¨í„´ ì‚¬ìš© ì—¬ë¶€
        vo_pattern_used = contains_vo_method(output)
        result['vo_pattern_used'] = vo_pattern_used

        # 4. Map ê´€ë ¨ ì½”ë“œ ë‚¨ì•„ ìˆëŠ”ì§€ ì—¬ë¶€
        map_pattern_remaining = contains_map_ops(output)
        result['map_pattern_remaining'] = map_pattern_remaining

        # 5. AST êµ¬ì¡° ë¹„êµ
        try:
            ast_output = get_ast(output_no_comments)
            ast_gt = get_ast(gt_no_comments)
            ast_match = ast_output == ast_gt
        except Exception as e:
            print(f"âš ï¸ AST parsing error: {e}")
            ast_match = False
        result['ast_match'] = ast_match

        # 6. ë¬¸ìì—´ ìœ ì‚¬ë„
        similarity = SequenceMatcher(None, output.strip(), gt_code.strip()).ratio()
        result['similarity'] = similarity

        # 7. ìµœì¢… ì ìˆ˜ ê³„ì‚°
        score = 0
        score += 40 if exact_match else 0
        score += 20 if comment_ignored_match else 0
        score += 10 if vo_pattern_used else 0
        score += 10 if not map_pattern_remaining else 0
        score += 10 if ast_match else 0
        score += 10 * similarity  # similarity: 0~1
        final_score = min(100, round(score))
        result['final_score'] = final_score

        # ìµœì¢… ìš”ì•½ ì¶œë ¥
        print(f"âœ… Evaluation result:")
        for key, value in result.items():
            print(f"  {key}: {value}")

        return result
    
    def _print_summary(self, results: list[dict[str, any]]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥ (Module ëª¨ë“œ)"""
        
        total = len(results)
        correct = sum(1 for r in results if r.get('is_correct', False))
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š MODULE CONVERSION SUMMARY")
        print(f"{'='*60}")
        print(f"Total functions: {total}")
        print(f"Successful conversions: {correct}")
        print(f"Failed conversions: {total - correct}")
        print(f"Success rate: {correct/total*100:.1f}%" if total > 0 else "N/A")
        
        if total - correct > 0:
            print(f"\nâŒ Failed functions:")
            for result in results:
                if not result.get('is_correct', False):
                    error = result.get('error', 'Incorrect output')
                    print(f"  - {error}")

    def _print_page_summary(self, result: dict[str, any]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥ (Page ëª¨ë“œ)"""
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š PAGE CONVERSION SUMMARY")
        print(f"{'='*60}")
        print(f"Conversion mode: {result.get('mode', 'page')}")
        print(f"Success: {'âœ…' if result.get('is_correct', False) else 'âŒ'}")
        print(f"Iterations used: {result.get('iterations', 1)}")
        print(f"Original code length: {len(result.get('original_code', ''))}")
        print(f"Converted code length: {len(result.get('converted_code', ''))}")

    def convert_line_by_line(self, data: dict[str, any]) -> list[dict[str, any]]:
        """ë¼ì¸ë³„ ë³€í™˜ (Line ë‹¨ìœ„)"""
        
        results = []
        contexts = data['contexts']
        java_code = data['java_code']
        gt_java_code = data['gt_java_code']
        
        # ì½”ë“œë¥¼ ë¼ì¸ë³„ë¡œ ë¶„í• 
        java_lines = java_code.splitlines()
        gt_java_lines = gt_java_code.splitlines() if gt_java_code else java_lines
        
        # GT ë¼ì¸ ìˆ˜ê°€ ë¶€ì¡±í•œ ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€
        if len(gt_java_lines) < len(java_lines):
            gt_java_lines += [''] * (len(java_lines) - len(gt_java_lines))
        elif len(java_lines) < len(gt_java_lines):
            java_lines += [''] * (len(gt_java_lines) - len(java_lines))

        print(f"ğŸ“ Converting {len(java_lines)} lines... (Line Mode)")

        for i, (line, gt_line) in enumerate(zip(java_lines, gt_java_lines)):
            print(f"\n{'='*60}")
            print(f"Converting line {i+1}/{len(java_lines)}")
            print(f"Original line: {line}")
            print(f"{'='*60}")
            
            # ë¹ˆ ë¼ì¸ì´ë‚˜ ì£¼ì„ë§Œ ìˆëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸° (ì˜µì…˜)
            if self.skip_non_map and (not line.strip() or line.strip().startswith('//')):
                print("â„¹ï¸  Empty line or comment. Skipping conversion.")
                results.append({
                    'original_code': line,
                    'converted_code': line,
                    'is_correct': True,
                    'iterations': 0,
                    'ground_truth': gt_line,
                    'skipped': True
                })
                continue
            
            try:
                contexts = self._build_contexts(contexts, line)
                result = self.convert_code(contexts, line, gt_line)
                results.append(result)

            except Exception as e:
                print(f"âŒ Error converting line {i+1}/{len(java_lines)}: {e}")
                results.append({
                    'original_code': line,
                    'error': str(e),
                    'is_correct': False,
                    'ground_truth': gt_line
                })
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        self._print_line_summary(results)
        
        return results

    def _print_line_summary(self, results: list[dict[str, any]]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥ (Line ëª¨ë“œ)"""
        
        total = len(results)
        correct = sum(1 for r in results if r.get('is_correct', False))
        skipped = sum(1 for r in results if r.get('skipped', False))
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š LINE CONVERSION SUMMARY")
        print(f"{'='*60}")
        print(f"Total lines: {total}")
        print(f"Successful conversions: {correct}")
        print(f"Failed conversions: {total - correct}")
        print(f"Skipped lines: {skipped}")
        print(f"Success rate: {correct/total*100:.1f}%" if total > 0 else "N/A")
        
        if total - correct > 0:
            print(f"\nâŒ Failed lines:")
            failed_count = 0
            for i, result in enumerate(results):
                if not result.get('is_correct', False) and not result.get('skipped', False):
                    failed_count += 1
                    if failed_count <= 5:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                        error = result.get('error', 'Incorrect output')
                        original = result.get('original_code', '')[:50]
                        print(f"  - Line {i+1}: {original}... | Error: {error}")
                    elif failed_count == 6:
                        print(f"  - ... and {total - correct - 5} more failed lines")


def create_parser():
    """argparse íŒŒì„œ ìƒì„±"""
    parser = argparse.ArgumentParser(
        description="Map-based implementationì„ VO-based implementationìœ¼ë¡œ ë³€í™˜",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  %(prog)s                                    # ê¸°ë³¸ ë³€í™˜ (module ëª¨ë“œ, ëª¨ë“  í•¨ìˆ˜)
  %(prog)s --mode module --use-diff           # Module ëª¨ë“œ + Diff ì •ë³´ í¬í•¨
  %(prog)s --mode page --use-reflextion       # Page ëª¨ë“œ + í”¼ë“œë°± ê¸°ë°˜ ê°œì„ 
  %(prog)s --mode page --use-diff --use-reflextion    # Page ëª¨ë“œ + Diff + í”¼ë“œë°±
  %(prog)s --mode page --use-api-rag    # Page ëª¨ë“œ + RAG ì‚¬ìš©
  %(prog)s --context sample_input.txt --java Sample.java --iterations 5
        """
    )
    
    parser.add_argument(
        '--model',
        type=str,
        default='qwen2.5-coder:7b',
        help='ì‚¬ìš©í•  ëª¨ë¸ ì§€ì • (ê¸°ë³¸: qwen2.5-coder:7b)'
    )
    
    parser.add_argument(
        '--mode',
        choices=['module', 'page', 'line'],
        default='module',
        help='ë³€í™˜ ëª¨ë“œ ì„ íƒ: module (í•¨ìˆ˜ë³„ ë³€í™˜), page (ì „ì²´ í˜ì´ì§€ ë³€í™˜), ë˜ëŠ” line (ë¼ì¸ë³„ ë³€í™˜) (ê¸°ë³¸: module)'
    )
    
    parser.add_argument(
        '--use-diff',
        action='store_true',
        help='ë³€í™˜ ì‹œ diff ì •ë³´ í¬í•¨'
    )
    
    parser.add_argument(
        '--use-reflextion',
        action='store_true',
        help='í”¼ë“œë°± ê¸°ë°˜ ë°˜ë³µ ê°œì„  ì‚¬ìš©'
    )

    parser.add_argument(
        '--use-prompt-normalization',
        action='store_true',
        help='í”„ë¡¬í”„íŠ¸ ì •ê·œí™” ì‚¬ìš©'
    )

    parser.add_argument(
        '--use-prefix-output',
        action='store_true',
        # default=True,
        help='ì¶œë ¥ í”„ë¡¬í”„íŠ¸ì— ```java ì½”ë“œ ë¸”ë¡ ì‚¬ìš© (ê¸°ë³¸: True)'
    )

    parser.add_argument(
        '--use-vo-generator',
        action='store_true',
        help='VO ìƒì„±ê¸° ì‚¬ìš©'
    )

    parser.add_argument(
        '--vo-package',
        type=str,
        default=None,
        help='VO íŒŒì¼ ê²½ë¡œ'
    )

    parser.add_argument(
        '--vo-class-name',
        type=str,
        default=None,
        help='VO í´ë˜ìŠ¤ ì´ë¦„'
    )

    parser.add_argument(
        '--project-root',
        type=str,
        default=None,
        help='í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ'
    )

    parser.add_argument(
        '--vo-file',
        type=str,
        default=None,
        help='VO íŒŒì¼ ê²½ë¡œ'
    )

    parser.add_argument(
        '--use-api-rag',
        action='store_true',
        help='Proworks5 api RAG ì‚¬ìš©'
    )

    parser.add_argument(
        '--use-case-rag',
        action='store_true',
        help='Case RAG ì‚¬ìš©'
    )
    
    parser.add_argument(
        '--max-line-limit-offset', '-mlo',
        type=int,
        default=None,
        help='Agent ì‘ë‹µ ë¼ì¸ ìˆ˜ ì œí•œì„ ìœ„í•œ ì˜¤í”„ì…‹ (None=ì œí•œ ì—†ìŒ, ê¸°ë³¸: None)'
    )
    
    parser.add_argument(
        '--skip-non-map',
        action='store_true',
        help='Map ê´€ë ¨ ì½”ë“œê°€ ì—†ëŠ” ê²½ìš° ë³€í™˜ ê±´ë„ˆë›°ê¸°'
    )
    
    parser.add_argument(
        '--context',
        type=str,
        # default="reflexion_poc/sample/sample_input.txt",
        # default="data/samples/vo.txt",
        default=None,
        help='ì»¨í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: reflexion_poc/sample/sample_input.txt)'
    )
    
    parser.add_argument(
        '--java',
        type=str,
        default="data/samples/file_sample_original/SampleTaskServiceImpl_in.java",
        help='ì…ë ¥ Java íŒŒì¼ ê²½ë¡œ'
    )
    
    parser.add_argument(
        '--gt',
        type=str,
        default="data/samples/file_sample_original/SampleTaskServiceImpl_out.java",
        help='Ground truth Java íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: ì…ë ¥ íŒŒì¼ê³¼ ë™ì¼)'
    )

    parser.add_argument(
        '--iterations', '-i',
        type=int,
        default=1,
        help='ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: 1)'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='ìƒì„¸ ì¶œë ¥ ëª¨ë“œ'
    )
    
    return parser


def run_conversion(config: ConversionConfig) -> str | None:
    """ë³€í™˜ ì‹¤í–‰ í•¨ìˆ˜
    
    Returns:
        str | None: ë³€í™˜ëœ ì½”ë“œ (ì˜¤ë¥˜ ë°œìƒì‹œ None ë°˜í™˜)
    """
    try:
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        context_path = config.context
        input_java_path = config.java
        gt_java_path = config.gt or input_java_path  # gtê°€ ì—†ìœ¼ë©´ ì…ë ¥ íŒŒì¼ ì‚¬ìš©

        # ë³€í™˜ê¸° ì´ˆê¸°í™”
        converter = AIConverter(
            agent=Agent(config.model, verbose=config.verbose),
            use_diff=config.use_diff,
            use_reflextion=config.use_reflextion,
            use_prompt_normalization=config.use_prompt_normalization,
            use_prefix_output=config.use_prefix_output,
            use_vo_generator=config.use_vo_generator,
            vo_file=config.vo_file,
            use_api_rag=config.use_api_rag,
            use_case_rag=config.use_case_rag,
            iterations=config.iterations,
            skip_non_map=config.skip_non_map,
            max_line_limit_offset=config.max_line_limit_offset,
            project_root=config.project_root,
            vo_package=config.vo_package,
            vo_class_name=config.vo_class_name
        )
        
        # íŒŒì¼ ë¡œë“œ
        if config.verbose:
            print("ğŸ“ Loading files...")
            print(f"  Context: {context_path}")
            print(f"  Java: {input_java_path}")
            print(f"  GT: {gt_java_path}")
            print(f"  Mode: {config.mode}")
        
        data = converter.load_java_files(input_java_path, context_path, gt_java_path)
        
        # ëª¨ë“œì— ë”°ë¥¸ ë³€í™˜ ì‹¤í–‰
        if config.mode == 'module':
            print(f"ğŸš€ Converting all Module... (Module Mode)")
            results = converter.convert_all_modules(data)
            
            print(f"\nğŸ‰ Module conversion completed!")
            
            # ê²°ê³¼ ìš”ì•½
            if len(results) > 1:
                successful = sum(1 for r in results if r.get('is_correct', False))
                print(f"ğŸ“Š Success rate: {successful}/{len(results)} ({successful/len(results)*100:.1f}%)")
            
            # ëª¨ë“  ëª¨ë“ˆì˜ ë³€í™˜ëœ ì½”ë“œë¥¼ í•©ì¹˜ê¸°
            converted_codes = []
            for result in results:
                converted_codes.append(result.get('converted_code'))
            
            return '\n\n'.join(converted_codes)
                
        elif config.mode == 'page':
            print(f"ğŸ“„ Converting entire page... (Page Mode)")
            result = converter.convert_whole_page(data)
            
            print(f"\nğŸ‰ Page conversion completed!")
            
            # ê²°ê³¼ ìš”ì•½
            converter._print_page_summary(result)
            
            return result.get('converted_code')
        
        elif config.mode == 'line':
            print(f"ğŸ“ Converting line by line... (Line Mode)")
            results = converter.convert_line_by_line(data)
            
            print(f"\nğŸ‰ Line-by-line conversion completed!")
            
            # ê²°ê³¼ ìš”ì•½
            if len(results) > 1:
                successful = sum(1 for r in results if r.get('is_correct', False))
                print(f"ğŸ“Š Success rate: {successful}/{len(results)} ({successful/len(results)*100:.1f}%)")
            
            # ëª¨ë“  ë¼ì¸ì˜ ë³€í™˜ëœ ì½”ë“œë¥¼ í•©ì¹˜ê¸°
            converted_lines = []
            for result in results:
                converted_lines.append(result.get('converted_code'))
            
            return '\n'.join(converted_lines)
        
    except FileNotFoundError as e:
        print(f"âŒ File not found: {e}")
        print("ğŸ’¡ Make sure the required files exist:")
        print(f"  - Context: {config.context}")
        print(f"  - Input Java: {config.java}")
        return None
        
    except Exception as e:
        print(f"âŒ Error during conversion: {e}")
        if config.verbose:
            import traceback
            traceback.print_exc()
        return None


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # argparse ì„¤ì •
    parser = create_parser()
    args = parser.parse_args()
    
    # argparse Namespaceë¥¼ pydantic ëª¨ë¸ë¡œ ë³€í™˜ (ìë™ ìœ íš¨ì„± ê²€ì‚¬ í¬í•¨)
    config = ConversionConfig(**vars(args))
    
    # ë³€í™˜ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜
    result = run_conversion(config)
    
    return result


if __name__ == "__main__":
    main()