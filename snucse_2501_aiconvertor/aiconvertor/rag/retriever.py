from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
import statistics


class ApiRetriever:
    """ë²¡í„° ì €ì¥ì†Œë¥¼ ì´ìš©í•œ ë¬¸ì„œ ê²€ìƒ‰ í´ë˜ìŠ¤"""
    
    def __init__(self, vectorstore_path: str, embedding_model_name: str):
        """
        Args:
            vectorstore_path: ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ
            embedding_model_name: ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        """
        self.vectorstore_path = vectorstore_path
        self.embedding_model_name = embedding_model_name
        self.embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)
        self.vectorstore = None
        self._load_vectorstore()
    
    def _load_vectorstore(self):
        """ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ"""
        self.vectorstore = FAISS.load_local(
            self.vectorstore_path, 
            self.embedding_model, 
            allow_dangerous_deserialization=True
        )
    
    def analyze_chunks(self):
        """ì²­í¬ ë¶„ì„ ì •ë³´ ì¶œë ¥"""
        if not self.vectorstore:
            raise ValueError("ë²¡í„°ìŠ¤í† ì–´ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        docs = self.vectorstore.docstore._dict.values()
        lengths = [len(doc.page_content) for doc in docs]

        print(f"ì´ ì²­í¬ ìˆ˜: {len(lengths)}")
        print(f"ìµœëŒ€ ê¸¸ì´: {max(lengths)}")
        print(f"ìµœì†Œ ê¸¸ì´: {min(lengths)}")
        print(f"í‰ê·  ê¸¸ì´: {int(statistics.mean(lengths))}")
        print(f"ì¤‘ì•™ê°’ ê¸¸ì´: {int(statistics.median(lengths))}")
        print("\nğŸ“˜ ì²­í¬:")
        for i, doc in enumerate(list(docs)[:]):
            print(f"[{i}] ê¸¸ì´={len(doc.page_content)} --------------------------------------")
            print(doc.page_content[:] + "\n---\n")
    
    def query(
        self,
        query: str, 
        k: int = 3,
        similarity_threshold: float = 0.9,
        verbose: bool = False
    ) -> list[Document]:
        """
        ì¿¼ë¦¬ì— ëŒ€í•´ ìƒìœ„ kê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ë°˜í™˜ (ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒë§Œ í•„í„°ë§)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
            
        Returns:
            List[(Document, similarity_score)]: í•„í„°ë§ëœ ë¬¸ì„œì™€ ìœ ì‚¬ë„ ë¦¬ìŠ¤íŠ¸
        """
        if not self.vectorstore:
            raise ValueError("ë²¡í„°ìŠ¤í† ì–´ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        results = self.vectorstore.similarity_search_with_score(query, k=k)
        
        # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜í•˜ê³  ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë§Œ í•„í„°ë§
        filtered_results = []
        for doc, distance in results:
            # ë³€í™˜ ì˜µì…˜ë“¤:
            # 1) 1/(1+d) - ë¶€ë“œëŸ¬ìš´ ê°ì†Œ
            # 2) exp(-d) - ì§€ìˆ˜ì  ê°ì†Œ
            # 3) max(0, 1-d) - ì„ í˜• ê°ì†Œ
            similarity = 1 - distance / 100
            if similarity >= similarity_threshold:
                filtered_results.append((doc, similarity))

        if verbose:
            print(f"ì „ì²´ ê²°ê³¼ ìˆ˜: {len(results)}, í•„í„°ë§ëœ ê²°ê³¼ ìˆ˜: {len(filtered_results)}")
            for i, (doc, similarity) in enumerate(filtered_results):
                print(f"[{i}] ìœ ì‚¬ë„: {similarity:.4f}")
                print(f"ë³¸ë¬¸: {doc.page_content}")
                # ë²¡í„° reconstruct
                # FAISS indexì— ì €ì¥ëœ ë²¡í„° id = i ì¼ ìˆ˜ ìˆìŒ (ì£¼ì˜: í‚¤ì™€ ìˆœì„œ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                # idë¥¼ ì°¾ìœ¼ë ¤ë©´ `index_to_docstore_id` í™œìš©
                doc_id = list(self.vectorstore.index_to_docstore_id.keys())[i]
                vec = self.vectorstore.index.reconstruct(i)
                print(f"í‚¤: {doc_id}")
                print(f"ë²¡í„°: {vec[:5]}...")  # ì¼ë¶€ë§Œ ì¶œë ¥
                print("---")
        
        return filtered_results
    
    def get_prompt(self, query: str, k: int = 3, similarity_threshold: float = 0.9) -> str | None:
        """ì¿¼ë¦¬ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        results = self.query(query, k=k, similarity_threshold=similarity_threshold, verbose=False)
        
        if not results:
            return None
        
        prompt_parts = [f"<related_documents>"]
        
        for i, (doc, similarity) in enumerate(results):
            prompt_parts.append(f"[{i}] similarity: {similarity:.4f}")
            prompt_parts.append(f"```java")
            prompt_parts.append(f"{doc.page_content}")
            prompt_parts.append(f"```")
            prompt_parts.append("")  # ë¹ˆ ì¤„ ì¶”ê°€
        
        prompt_parts.append(f"</related_documents>")
        
        return "\n".join(prompt_parts)


if __name__ == "__main__":
    embedding_model_name = "microsoft/codebert-base" #"BAAI/bge-base-en-v1.5"
    # embedding_model_name = "nlpai-lab/KURE-v1"
    # embedding_model_name = "monologg/kobert"
    vectorstore_path = f"data/db/proworks5_vectorstore_{embedding_model_name.split('/')[-1]}"

    # ApiRetriever í´ë˜ìŠ¤ ì‚¬ìš©
    retriever = ApiRetriever(vectorstore_path, embedding_model_name)
    # retriever.analyze_chunks()

    # ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
    query = "public void deleteEmpXDA(Map doc) throws Exception {"
    # results = retriever.query(query, k=5, similarity_threshold=0.71, verbose=True)

    print(retriever.get_prompt(query, k=5, similarity_threshold=0.71))
